{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a stream of words, then skipgram pairs, then training batches based on the input file.\n",
    "These streams are built on-demand (see: Python generators) so the whole file does not have to be read into memory at once, allowing training on big datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stream(file_name, buf_bytes=1000000):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        chars = f.read(buf_bytes)\n",
    "        max_index = 1\n",
    "        while max_index != 0:\n",
    "            max_index = 0\n",
    "            for match in re.finditer(\"([a-z]+)\\\\s\", chars):\n",
    "                yield match.group(1)\n",
    "                max_index = match.end(0)\n",
    "            chars = chars[max_index:] + f.read(buf_bytes)\n",
    "        if re.match(\"[a-z]+\", chars):\n",
    "            yield chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_statistics(word_stream):\n",
    "    words = list(word_stream)\n",
    "    words_counts = {w: 0 for w in words}\n",
    "    for w in words:\n",
    "        words_counts[w] += 1\n",
    "    words_unique = list(set(words))\n",
    "    words_unique = sorted(words_unique, key = lambda w : -words_counts[w])\n",
    "    words_probs = [words_counts[w] / float(len(words)) for w in words_unique]\n",
    "    words_mapping = {}\n",
    "    for i, w in enumerate(words_unique):\n",
    "        words_mapping[i] = w\n",
    "        words_mapping[w] = i\n",
    "    return words_unique, words_probs, words_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_stream(word_stream, words_mapping):\n",
    "    for w in word_stream:\n",
    "        yield words_mapping[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_pair_stream(stream, window_size):\n",
    "    buffer = list(itertools.islice(stream, window_size + 1))\n",
    "    pointer = 0\n",
    "    while pointer < len(buffer):\n",
    "        for i in range(-window_size, window_size + 1):\n",
    "            other = pointer + i\n",
    "            if other < 0 or other >= len(buffer) or other == pointer:\n",
    "                continue\n",
    "            yield (buffer[pointer], buffer[other])\n",
    "        # append next of stream to head of buffer (if available)\n",
    "        try:\n",
    "            buffer.append(next(stream))\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        # move center point to the head\n",
    "        pointer += 1\n",
    "        # remove from tail if no longer needed\n",
    "        if pointer > window_size:\n",
    "            buffer.pop(0)\n",
    "            pointer -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_batch_stream(skipgram_stream, batch_size, cache_size=100000):\n",
    "    cache = list(itertools.islice(skipgram_stream, cache_size))\n",
    "    while True:\n",
    "        for i in range(0, len(cache) - batch_size + 1, batch_size):\n",
    "            block = cache[i:i + batch_size]\n",
    "            inputs = [pair[0] for pair in block]\n",
    "            targets = [pair[1] for pair in block]\n",
    "            yield (inputs, targets)\n",
    "        cache = cache[len(cache) - (len(cache) % batch_size):]\n",
    "        new_elements = list(itertools.islice(skipgram_stream, cache_size))\n",
    "        cache += new_elements\n",
    "        if len(new_elements) == 0:\n",
    "            break\n",
    "    if len(cache) > 0:\n",
    "        inputs = [pair[0] for pair in cache]\n",
    "        targets = [pair[1] for pair in cache]\n",
    "        yield (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_stream(text_file_name, words_mapping, window_size, batch_size):\n",
    "    w_stream = word_stream(text_file_name)\n",
    "    i_stream = int_stream(w_stream, words_mapping)\n",
    "    sgp_stream = skipgram_pair_stream(i_stream, window_size)\n",
    "    batch_stream = training_batch_stream(sgp_stream, batch_size)\n",
    "    return batch_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the TensorFlow execution graph for the neural network. The network is fed a list (batch) of input classes and a list of target classes (in the form of 1d vectors of word indices). The result is a 1d vector of the loss for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(vocab_size, embedding_size, num_samples):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # input and target output are passed into the network via these placeholders and feed_dict\n",
    "    inputs_placeholder = tf.placeholder(shape=(None, ), dtype=tf.int32)\n",
    "    targets_placeholder = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "    \n",
    "    weights_initializer = tf.random_uniform_initializer(minval=-0.05, maxval=0.05)\n",
    "    # weights of input -> hidden (embeddings matrix)\n",
    "    weights_1 = tf.get_variable(\"weights_1\", shape=(vocab_size, embedding_size),\n",
    "                                dtype=tf.float32, initializer=weights_initializer)\n",
    "    # weights of hidden -> output\n",
    "    #weights_2 = tf.get_variable(\"weights_2\", shape=(embedding_size, vocab_size),\n",
    "    #                            dtype=tf.float32, initializer=weights_initializer)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Network input is a 1d vector of word indices\n",
    "    # convert to a 2d matrix of 1-hot vectors\n",
    "    #net_inputs = tf.one_hot(inputs_placeholder, depth=vocab_size)\n",
    "    # multiply with embedding matrix\n",
    "    #net_mul1 = tf.matmul(net_inputs, weights_1)\n",
    "    net_mul1 = tf.nn.embedding_lookup(weights_1, inputs_placeholder)\n",
    "    \n",
    "    # use sampled softmax loss (number of samples specified)\n",
    "    if num_samples is not None:\n",
    "        weights_2 = tf.get_variable(\"weights_2\", shape=(vocab_size, embedding_size),\n",
    "                                    dtype=tf.float32, initializer=weights_initializer)\n",
    "        zero_bias = tf.zeros(vocab_size, dtype=tf.float32)\n",
    "        #w2_transposed = tf.transpose(weights_2)\n",
    "        loss = tf.nn.sampled_softmax_loss(inputs=net_mul1, weights=weights_2, biases=zero_bias,\n",
    "                                          labels=targets_placeholder, num_sampled=num_samples, \n",
    "                                          num_classes=vocab_size)\n",
    "    # use regular softmax loss (no number of samples specified)\n",
    "    else:\n",
    "        weights_2 = tf.get_variable(\"weights_2\", shape=(embedding_size, vocab_size),\n",
    "                                    dtype=tf.float32, initializer=weights_initializer)\n",
    "        net_output = tf.matmul(net_mul1, weights_2)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets_placeholder,\n",
    "                                                              logits=net_output)\n",
    "    \n",
    "    # return only what is necessary\n",
    "    # input and target placeholders are for feeding data\n",
    "    # loss is connected to an optimizer which works its way back to the weights to adjust them\n",
    "    # weights_1 is the embedding matrix containing the word embeddings\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return (inputs_placeholder, targets_placeholder, loss, weights_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(inputs_placeholder, targets_placeholder, weights_1, loss,\n",
    "                  train_stream_builder, epochs, learning_rate, total_pairs):\n",
    "    print(\"training started\")\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    time_baseline = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        epoch_losses = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            batch_count, pairs_count, sum_loss = 0, 0, 0.0\n",
    "            for batch_inputs, batch_targets in train_stream_builder():\n",
    "                batch_targets_b = [[t] for t in batch_targets]\n",
    "                #print(batch_inputs)\n",
    "                #print(batch_targets_b)\n",
    "                feed_dict = {inputs_placeholder: batch_inputs, targets_placeholder: batch_targets_b}\n",
    "                time_start = time.time()\n",
    "                optimizer_stats, batch_loss = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                time_end = time.time()\n",
    "                batch_count += 1\n",
    "                pairs_count += len(batch_inputs)\n",
    "                sum_loss += batch_loss\n",
    "                if time.time() - time_baseline >= 10.0:\n",
    "                    status_info = \"epoch {}, {}/{} pairs, avg loss: {:.5f}, time per batch: {:.5f}s\"\n",
    "                    status_info = status_info.format(epoch, pairs_count, total_pairs,\n",
    "                                                     sum_loss / float(batch_count),\n",
    "                                                     time_end - time_start)\n",
    "                    print(status_info)\n",
    "                    #write_analogy_accuracy(epoch, sess.run(weights_1))\n",
    "                    time_baseline = time.time()\n",
    "            epoch_losses.append((epoch, sum_loss / float(batch_count)))\n",
    "            write_losses(epoch_losses, loss_file_name())\n",
    "            write_analogy_accuracy(epoch, sess.run(weights_1))\n",
    "        print(\"training complete\")\n",
    "        return sess.run(weights_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(words, embeddings_matrix, dot_size=1):\n",
    "    tsne = TSNE(n_components=2, random_state=1)\n",
    "    embeddings_matrix_2d = tsne.fit_transform(embeddings_matrix)\n",
    "    %matplotlib notebook\n",
    "    plt.scatter(embeddings_matrix_2d[:,0], embeddings_matrix_2d[:,1], s=dot_size)\n",
    "    for i, word in enumerate(words):\n",
    "        plt.text(embeddings_matrix_2d[i][0], embeddings_matrix_2d[i][1], word)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_term(s):\n",
    "    term = []\n",
    "    for m in re.finditer(\"(\\\\+|-)?(\\\\w+)\", s):\n",
    "        word, symbol = m.group(2), m.group(1)\n",
    "        if symbol is None or symbol == \"+\":\n",
    "            factor = 1\n",
    "        elif symbol == \"-\":\n",
    "            factor = -1\n",
    "        else:\n",
    "            raise ValueError(\"invalid symbol\")\n",
    "        term.append((word, factor))\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_sum(words_mapping, embeddings_matrix, term):\n",
    "    vector = np.zeros(len(embeddings_matrix[0]), dtype=np.float32)\n",
    "    for word, factor in term:\n",
    "        vector += embeddings_matrix[words_mapping[word]] * factor\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities(words, embeddings_matrix, vector):\n",
    "    similarities = []\n",
    "    for i, word in enumerate(words):\n",
    "        embedding = embeddings_matrix[i]\n",
    "        similarity = embedding.dot(vector) / (np.linalg.norm(embedding) * np.linalg.norm(vector))\n",
    "        similarities.append((word, similarity))\n",
    "    return sorted(similarities, key = lambda s : -s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities_s(words, words_mapping, embeddings_matrix, s):\n",
    "    term = parse_term(s)\n",
    "    term_words = [t[0] for t in term]\n",
    "    vector = embedding_sum(words_mapping, embeddings_matrix, term)\n",
    "    similarities = cosine_similarities(words, embeddings_matrix, vector)\n",
    "    similarities = [s for s in similarities if s[0] not in term_words]\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_embeddings(words_mapping, embeddings, file_name):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            word = words_mapping[i]\n",
    "            print(word + \" \" + \" \".join(str(e) for e in embedding), file=f)\n",
    "\n",
    "def write_losses(epoch_losses, file_name):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        for epoch, avg_loss in epoch_losses:\n",
    "            print(str(epoch) + \" \" + str(avg_loss), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_file_name(epoch):\n",
    "    return \"accuracy_\" + text_file_name + \"_s\" + str(num_samples) + \"_e\" + str(epoch)\n",
    "\n",
    "def loss_file_name():\n",
    "    return \"losses_\" + text_file_name + \"_s\" + str(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analogy_questions(file_name):\n",
    "    questions = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f.read().splitlines():\n",
    "            line_parts = line.split(\" \")\n",
    "            if len(line_parts) != 4:\n",
    "                continue\n",
    "            question = line_parts[2] + \"-\" + line_parts[0]  + \"+\" + line_parts[1]\n",
    "            answer = line_parts[3]\n",
    "            questions.append((question, answer))\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy_accuracy(words, words_mapping, embeddings, questions, score_top_n, sample_size = None):\n",
    "    if sample_size is not None:\n",
    "        questions = random.sample(questions, sample_size)\n",
    "    score_top_n = 10\n",
    "    scores = [0] * (score_top_n + 1)\n",
    "    for question, answer in questions:\n",
    "        answers = None\n",
    "        try:\n",
    "            answers = cosine_similarities_s(words, words_mapping, embeddings, question)\n",
    "            answers = sorted(answers, key = lambda a : -a[1])\n",
    "            answers = answers[:score_top_n]\n",
    "            #print(\"success for \" + question)\n",
    "            #print(\"answers: {}, correct answer: {}\".format(answers, answer))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if answers is None or answer not in answers:\n",
    "            scores[0] += 1\n",
    "        else:\n",
    "            index = answers.index(answer)\n",
    "            scores[index + 1] += 1\n",
    "    scores = [s / float(len(questions)) for s in scores]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_analogy_accuracy(epoch, embeddings):\n",
    "    scores = test_analogy_accuracy(words_unique, words_mapping, embeddings, analogy_questions, score_top_n, analogy_sample_size)\n",
    "    file_name = accuracy_file_name(epoch)\n",
    "    with open(file_name, \"w\") as f:\n",
    "        for i in range(1, len(scores)):\n",
    "            print(\"{}: {}\".format(i, scores[i]), file=f)\n",
    "        print(\"fail: \" + str(scores[0]), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_name = \"text8_small\"\n",
    "window_size = 5\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "num_samples = 1\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "score_top_n = 10\n",
    "analogy_sample_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_questions = load_analogy_questions(\"questions-words.txt\")\n",
    "words_unique, words_probs, words_mapping = vocabulary_statistics(word_stream(text_file_name))\n",
    "train_pairs_estimated = sum(2 * window_size for w in word_stream(text_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  16774\n",
      "WARNING:tensorflow:From /home/stud1/hplatzer/progs/word2vec_env/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:1310: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "training started\n",
      "epoch 2, 128/1672850 pairs, avg loss: 0.36747, time per batch: 0.00097s\n",
      "epoch 3, 128/1672850 pairs, avg loss: 0.63126, time per batch: 0.00095s\n",
      "epoch 4, 128/1672850 pairs, avg loss: 1.20161, time per batch: 0.00128s\n",
      "epoch 5, 128/1672850 pairs, avg loss: 1.83155, time per batch: 0.00113s\n",
      "epoch 6, 128/1672850 pairs, avg loss: 1.24127, time per batch: 0.00108s\n",
      "epoch 7, 128/1672850 pairs, avg loss: 0.67118, time per batch: 0.00213s\n",
      "epoch 8, 128/1672850 pairs, avg loss: 0.56468, time per batch: 0.00095s\n",
      "epoch 9, 128/1672850 pairs, avg loss: 1.34794, time per batch: 0.00103s\n",
      "epoch 10, 128/1672850 pairs, avg loss: 0.74459, time per batch: 0.00099s\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "train_stream_builder = lambda : build_training_stream(text_file_name, words_mapping, window_size, batch_size)\n",
    "vocab_size = len(words_unique)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "network = build_network(vocab_size, embedding_size, num_samples)\n",
    "inputs_placeholder, targets_placeholder, loss, weights_1 = network\n",
    "embeddings_matrix = train_network(inputs_placeholder, targets_placeholder, weights_1,\n",
    "                                  loss, train_stream_builder, epochs, learning_rate,\n",
    "                                  train_pairs_estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_analogy_accuracy(words_unique, words_mapping, embeddings_matrix, analogy_questions, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_embeddings(words, embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_analogies_quality(words, embeddings_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
